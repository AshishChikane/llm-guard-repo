# Toxicity Scanner

It is designed to assess the toxicity level of the content generated by language models, acting as a safeguard against
potentially harmful or offensive output.

## Attack

Language models, when interacting with users, can sometimes produce responses that may be deemed toxic or inappropriate.
This poses a risk, as such output can perpetuate harm or misinformation. By monitoring and classifying the model's
output, potential toxic content can be flagged and handled appropriately.

## How it works

The scanner employs the [nicholasKluge/ToxicityModel](https://huggingface.co/nicholasKluge/ToxicityModel) from
HuggingFace to evaluate the generated text's toxicity level.

- A negative score (approaching 0) flags the content as toxic.

- A positive score (approaching 1) indicates non-toxic content.

The calculated toxicity score is then juxtaposed against a pre-set threshold. Outputs that cross this threshold are
marked as toxic.

## Usage

```python
from llm_guard.output_scanners import Toxicity

scanner = Toxicity(threshold=0.7)
sanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)
```

## Optimizations

### ONNX

The scanner can be optimized by converting the model to ONNX format. This can be done by setting the `use_onnx`.

To enable it, install the `onnxruntime` package:

```sh
pip install onnx onnxruntime optimum[onnxruntime]
```

## Benchmarks

Environment:

- Platform: Amazon Linux 2
- Python Version: 3.11.6

Run the following script:

```sh
python benchmarks/run.py output Toxicity
```

Results:

| Instance          | Time taken, s | Characters per Second | Total Length Processed |
|-------------------|---------------|-----------------------|------------------------|
| inf1.xlarge (AWS) | 0.111         | 1961.58               | 217                    |
| m5.large (AWS)    | 0.162         | 1336.85               | 217                    |
