import logging
from typing import Any, Dict, List, Optional

import langchain
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.callbacks.stdout import StdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.llms.base import LLM
from langchain.prompts import PromptTemplate
from langchain.pydantic_v1 import Extra, root_validator
from langchain.schema.language_model import BaseLanguageModel

logger = logging.getLogger(__name__)


class LLMGuardPromptInvalidException(Exception):
    """Exception to raise when a llm-guard marks prompt invalid."""


class LLMGuardOutputInvalidException(Exception):
    """Exception to raise when a llm-guard marks result invalid."""


class LLMGuard(LLM):
    """An LLM wrapper that uses LLMGuard to sanitize prompts.

    Wraps another LLM and sanitizes prompts before passing it to the LLM, then
        de-sanitizes the response.

    To use, you should have the ``llm-guard`` python package installed.

    Example:
        .. code-block:: python

            from langchain.chat_models import ChatOpenAI

            lg_llm = LLMGuard(base_llm=ChatOpenAI())
    """

    base_llm: BaseLanguageModel
    """The base LLM to use."""

    input_scanners: List[Any] = []
    """The input scanners to use."""
    output_scanners: List[Any] = []
    """The output scanners to use."""
    raise_error: bool = True
    """Whether to raise an error if the LLMGuard marks the prompt or result invalid."""
    fail_fast: bool = True
    """Whether to fail fast if the LLMGuard marks the prompt or result invalid."""

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validates that the LLMGuard Python package exist."""
        try:
            import llm_guard
        except ImportError:
            raise ImportError(
                "Could not import the `llm-guard` Python package, "
                "please install it with `pip install llm-guard`."
            )
        if llm_guard.__package__ is None:
            raise ValueError(
                "Could not properly import `llm-guard`, " "llm-guard.__package__ is None."
            )

        return values

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call base LLM with scanning the prompt and scanning the output after.

        Args:
            prompt: The prompt to pass into the model.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = lg_llm("Tell me a joke.")
        """
        import llm_guard

        run_manager or CallbackManagerForLLMRun.get_noop_manager()

        # scan and sanitize the prompt
        sanitized_prompt, results_valid, results_score = llm_guard.scan_prompt(
            self.input_scanners, prompt, self.fail_fast
        )
        if any(not result for result in results_valid.values()):
            logger.warning(f"Prompt '{sanitized_prompt}' is not valid, scores: {results_score}")

            if self.raise_error:
                raise LLMGuardPromptInvalidException(
                    f"Prompt '{prompt}' is invalid based on scores {results_score}."
                )

        # call the LLM with the sanitized prompt and get the response
        llm_response = self.base_llm.predict(
            sanitized_prompt,
            stop=stop,
        )

        # scan the output to make sure it is valid
        sanitized_response_text, results_valid, results_score = llm_guard.scan_output(
            self.output_scanners, sanitized_prompt, llm_response, self.fail_fast
        )
        if any(not result for result in results_valid.values()):
            if self.raise_error:
                raise LLMGuardOutputInvalidException(
                    f"Output is invalid based on scores {results_score}."
                )

        return sanitized_response_text

    @property
    def _llm_type(self) -> str:
        """Return type of LLM.

        This is an override of the base class method.
        """
        return "llm_guard"


langchain.verbose = True
langchain.debug = True

prompt_template = "Make an SQL insert statement to add a new user to our database. Name is {name}. Email is {email1} "
"but also possible to contact him with {email2} email. Phone number is {phone} and "
"the IP address is {ip}. And credit card number is {credit_card}. "
"He works in {company}.",

from llm_guard.input_scanners import Anonymize, PromptInjection, TokenLimit, Toxicity
from llm_guard.output_scanners import Deanonymize, NoRefusal, Relevance, Sensitive
from llm_guard.vault import Vault

vault = Vault()

chain = LLMChain(
    prompt=PromptTemplate.from_template(prompt_template),
    llm=LLMGuard(
        base_llm=OpenAI(),
        input_scanners=[Anonymize(vault), Toxicity(), TokenLimit(), PromptInjection()],
        output_scanners=[Deanonymize(vault), NoRefusal(), Relevance(), Sensitive()],
        raise_error=False,
        fail_fast=True,
    ),
    verbose=True,
)

print(
    chain.predict(
        name="John Doe",
        email1="test@test.com",
        email2="hello@test.com",
        phone="555-123-4567",
        ip="192.168.1.100",
        credit_card="4567-8901-2345-6789",
        company="Test LLC",
        callbacks=[StdOutCallbackHandler()],
    )
)
