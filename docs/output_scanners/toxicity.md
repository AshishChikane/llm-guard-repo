# Toxicity Scanner

It is designed to assess the toxicity level of the content generated by language models, acting as a safeguard against
potentially harmful or offensive output.

## Attack

Language models, when interacting with users, can sometimes produce responses that may be deemed toxic or inappropriate.
This poses a risk, as such output can perpetuate harm or misinformation. By monitoring and classifying the model's
output, potential toxic content can be flagged and handled appropriately.

## How it works

The scanner employs the [unitary/unbiased-toxic-roberta](https://huggingface.co/unitary/unbiased-toxic-roberta) from
HuggingFace to evaluate the generated text's toxicity level.

## Usage

```python
from llm_guard.output_scanners import Toxicity

scanner = Toxicity(threshold=0.7)
sanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)
```

## Optimizations

### ONNX

The scanner can be optimized by converting the model to ONNX format. This can be done by setting the `use_onnx`.

To enable it, install the `onnxruntime` package:

```sh
pip install onnx onnxruntime optimum[onnxruntime]
```

## Benchmarks

Environment:

- Platform: Amazon Linux 2
- Python Version: 3.11.6

Run the following script:

```sh
python benchmarks/run.py output Toxicity
```

Results:

| Instance          | Time taken, s | Characters per Second | Total Length Processed |
|-------------------|---------------|-----------------------|------------------------|
| inf1.xlarge (AWS) | 0.111         | 1961.58               | 217                    |
| m5.large (AWS)    | 0.162         | 1336.85               | 217                    |
