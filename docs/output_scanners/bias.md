# Bias Detection Scanner

This scanner is designed to inspect the outputs generated by Language Learning Models (LLMs) to detect and evaluate
potential biases. Its primary function is to ensure that LLM outputs remain neutral and don't exhibit unwanted or
predefined biases.

## Attack

In the age of AI, it's pivotal that machine-generated content adheres to neutrality. Biases, whether intentional or
inadvertent, in LLM outputs can be misrepresentative, misleading, or offensive. The `Bias` scanner serves to address
this by detecting and quantifying biases in generated content.

## How it works

The scanner utilizes a model from
HuggingFace: [valurank/distilroberta-bias](https://huggingface.co/valurank/distilroberta-bias). This model is
specifically trained to detect biased statements in text. By examining a text's classification and score against a
predefined threshold, the scanner determines whether it's biased.

!!! note

    Supported languages: English

## Usage

```python
from llm_guard.output_scanners import Bias

scanner = Bias(threshold=0.5)
sanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)
```

## Optimizations

### ONNX

The scanner can be optimized by converting the model to ONNX format. This can be done by setting the `use_onnx`.

To enable it, install the `onnxruntime` package:

```sh
pip install llm-guard[onnxruntime]
```

## Benchmarks

Environment:

- Platform: Amazon Linux 2
- Python Version: 3.11.6

Run the following script:

```sh
python benchmarks/run.py output Bias
```

Results:

| Instance          | Time taken, s | Characters per Second | Total Length Processed |
|-------------------|---------------|-----------------------|------------------------|
| inf1.xlarge (AWS) | 0.034         | 3790.06               | 128                    |
| m5.large (AWS)    | 0.057         | 2242.91               | 128                    |
