# Ban Topics Scanner

It is designed to inspect the outputs generated by Language Learning Models and to flag or restrict responses that delve
into predefined banned topics, such as religion. This ensures that the outputs align with community guidelines and do
not drift into potentially sensitive or controversial areas.

## Attack

Even with controlled prompts, LLMs might produce outputs touching upon themes or subjects that are considered sensitive,
controversial, or outside the scope of intended interactions. Without preventive measures, this can lead to outputs that
are misaligned with the platform's guidelines or values.

## How it works

It relies on the capabilities of the model from
HuggingFace: [MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c).
This model identifies the topic or theme of an output, enabling the scanner to vet the content against a predefined list
of banned topics.

## Usage

```python
from llm_guard.output_scanners import BanTopics

scanner = BanTopics(topics=["violence"], threshold=0.5)
sanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)
```

## Benchmarks

Environment:

- Platform: Amazon Linux 2
- Python Version: 3.11.6

Run the following script:

```sh
python benchmarks/run.py output BanTopics
```

Results:

| Instance          | Time taken, s | Characters per Second | Total Length Processed |
|-------------------|---------------|-----------------------|------------------------|
| inf1.xlarge (AWS) | 0.448         | 198.84                | 89                     |
| m5.large (AWS)    | 0.775         | 114.8                 | 89                     |
