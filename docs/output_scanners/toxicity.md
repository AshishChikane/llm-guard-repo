# Toxicity Scanner

It is designed to assess the toxicity level of the content generated by language models, acting as a safeguard against
potentially harmful or offensive output.

## Attack

Language models, when interacting with users, can sometimes produce responses that may be deemed toxic or inappropriate.
This poses a risk, as such output can perpetuate harm or misinformation. By monitoring and classifying the model's
output, potential toxic content can be flagged and handled appropriately.

## How it works

The scanner employs the [nicholasKluge/ToxicityModel](https://huggingface.co/nicholasKluge/ToxicityModel) from
HuggingFace to evaluate the generated text's toxicity level.

- A negative score (approaching 0) flags the content as toxic.

- A positive score (approaching 1) indicates non-toxic content.

The calculated toxicity score is then juxtaposed against a pre-set threshold. Outputs that cross this threshold are
marked as toxic.

## Usage

```python
from llm_guard.output_scanners import Toxicity

scanner = Toxicity(threshold=0.7)
sanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)
```
